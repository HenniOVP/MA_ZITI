{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /workspace/.local/lib/python3.6/site-packages (1.1.5)\r\n",
      "Requirement already satisfied: numpy>=1.15.4 in /opt/conda/lib/python3.6/site-packages (from pandas) (1.19.4)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.6/site-packages (from pandas) (2.8.1)\r\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas) (2019.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".container { width:90% !important; }\n",
       ".rendered_html pre code {border: 0; background-color: #f4f4f4; display:inline-block;}\n",
       ".rendered_html code.language-cpp {border: 0; background-color: #f4f4f4; display:inline-block;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".container { width:90% !important; }\n",
    ".rendered_html pre code {border: 0; background-color: #f4f4f4; display:inline-block;}\n",
    ".rendered_html code.language-cpp {border: 0; background-color: #f4f4f4; display:inline-block;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_script_version = \"1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# as of Feb'20 there is a bug that segfaults ONNX shape inference if we\n",
    "# import pytorch before onnx, so we make sure to import onnx first\n",
    "import onnx  # NOQA\n",
    "\n",
    "import pkg_resources as pk\n",
    "from finn.custom_op.registry import getCustomOp\n",
    "from finn.core.onnx_exec import execute_onnx\n",
    "from finn.transformation.double_to_single_float import DoubleToSingleFloat\n",
    "from finn.transformation.infer_shapes import InferShapes\n",
    "from finn.transformation.move_reshape import RemoveCNVtoFCFlatten\n",
    "from finn.transformation.fold_constants import FoldConstants\n",
    "from finn.transformation.general import (\n",
    "    RemoveUnusedTensors,\n",
    "    RemoveStaticGraphInputs,\n",
    "    GiveReadableTensorNames,\n",
    "    GiveUniqueNodeNames,\n",
    ")\n",
    "from finn.transformation.streamline import Streamline\n",
    "from finn.transformation.lower_convs_to_matmul import LowerConvsToMatMul\n",
    "import finn.transformation.streamline.absorb as absorb\n",
    "from finn.transformation.streamline.reorder import MakeMaxPoolNHWC\n",
    "import finn.transformation.fpgadataflow.convert_to_hls_layers as to_hls\n",
    "from finn.transformation.fpgadataflow.create_dataflow_partition import (\n",
    "    CreateDataflowPartition,\n",
    ")\n",
    "from finn.transformation.fpgadataflow.insert_dwc import InsertDWC\n",
    "from finn.transformation.fpgadataflow.insert_tlastmarker import InsertTLastMarker\n",
    "from finn.transformation.fpgadataflow.prepare_ip import PrepareIP\n",
    "from finn.transformation.fpgadataflow.hlssynth_ip import HLSSynthIP\n",
    "from finn.transformation.fpgadataflow.replace_verilog_relpaths import (\n",
    "    ReplaceVerilogRelPaths,\n",
    ")\n",
    "from finn.transformation.fpgadataflow.create_stitched_ip import CreateStitchedIP\n",
    "from finn.transformation.fpgadataflow.set_exec_mode import SetExecMode\n",
    "from finn.transformation.fpgadataflow.prepare_cppsim import PrepareCppSim\n",
    "from finn.transformation.fpgadataflow.compile_cppsim import CompileCppSim\n",
    "from finn.transformation.fpgadataflow.make_pynq_driver import MakePYNQDriver\n",
    "from finn.transformation.fpgadataflow.make_deployment import DeployToPYNQ\n",
    "from finn.util.basic import pynq_part_map\n",
    "from finn.util.test import get_test_model_trained, load_test_checkpoint_or_skip\n",
    "from finn.transformation.fpgadataflow.annotate_resources import AnnotateResources\n",
    "from finn.transformation.fpgadataflow.prepare_rtlsim import PrepareRTLSim\n",
    "from finn.transformation.fpgadataflow.insert_fifo import InsertFIFO\n",
    "\n",
    "import brevitas.onnx as bo\n",
    "\n",
    "from finn.util.visualization import showInNetron\n",
    "import time\n",
    "from finn.core.modelwrapper import ModelWrapper\n",
    "\n",
    "from finn.util.vcd import get_all_stream_if_stats\n",
    "import finn.util.vcd as vcd\n",
    "import time\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "import pkg_resources as pk\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from finn.core.onnx_exec import execute_onnx\n",
    "from finn.core.throughput_test import throughput_test_remote, throughput_test_rtlsim\n",
    "from multiprocessing import Process, Lock\n",
    "\n",
    "from subprocess import TimeoutExpired\n",
    "from glob import glob\n",
    "from brevitas_examples.bnn_pynq.models.CNV import CNV\n",
    "import torch\n",
    "\n",
    "# one bit activation stuff\n",
    "from finn.transformation.bipolar_to_xnor import ConvertBipolarMatMulToXnorPopcount\n",
    "\n",
    "# new with 0.4\n",
    "from finn.transformation.fpgadataflow.make_zynq_proj import ZynqBuild\n",
    "from finn.transformation.infer_datatypes import InferDataTypes\n",
    "from finn.util.pytorch import ToTensor\n",
    "from finn.transformation.merge_onnx_models import MergeONNXModels\n",
    "from finn.core.datatype import DataType\n",
    "from finn.transformation.insert_topk import InsertTopK\n",
    "from finn.transformation.streamline.reorder import (\n",
    "    MakeMaxPoolNHWC,\n",
    "    MoveScalarLinearPastInvariants,\n",
    ")\n",
    "from finn.transformation.infer_data_layouts import InferDataLayouts\n",
    "from finn.transformation.fpgadataflow.annotate_cycles import AnnotateCycles\n",
    "from finn.analysis.fpgadataflow.dataflow_performance import dataflow_performance\n",
    "from finn.util.test import (\n",
    "    get_build_env,\n",
    "    load_test_checkpoint_or_skip,\n",
    "    get_example_input,\n",
    "    get_trained_network_and_ishape,\n",
    "    execute_parent,\n",
    "    get_topk,\n",
    ")\n",
    "from scipy.stats import linregress\n",
    "from finn.util.test import example_map\n",
    "import psutil\n",
    "import shutil\n",
    "\n",
    "# for autotuning\n",
    "from finn.analysis.fpgadataflow.exp_cycles_per_layer import exp_cycles_per_layer\n",
    "from finn.analysis.fpgadataflow.dataflow_performance import dataflow_performance\n",
    "from finn.analysis.fpgadataflow.res_estimation import res_estimation, res_estimation_complete\n",
    "import copy\n",
    "\n",
    "# For gzip\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Could get removed in when upgraded to newer version\n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "import warnings\n",
    "\n",
    "from finn.core.rtlsim_exec import rtlsim_exec\n",
    "from finn.util.basic import gen_finn_dt_tensor\n",
    "\n",
    "\n",
    "def throughput_test_remote(model, batchsize=1000, timeout=None):\n",
    "    \"\"\"Runs the throughput test for the given model remotely on the pynq board.\n",
    "    The metadata properties related to the pynq board have to be set.\n",
    "    Additionally a timeout for the SSH communication can be set.\n",
    "    Returns a dictionary with results of the throughput test. Returns None\n",
    "    if the test fails.\"\"\"\n",
    "\n",
    "    pynq_ip = model.get_metadata_prop(\"pynq_ip\")\n",
    "    pynq_port = int(model.get_metadata_prop(\"pynq_port\"))\n",
    "    pynq_username = model.get_metadata_prop(\"pynq_username\")\n",
    "    pynq_password = model.get_metadata_prop(\"pynq_password\")\n",
    "    pynq_target_dir = model.get_metadata_prop(\"pynq_target_dir\")\n",
    "    deployment_dir = model.get_metadata_prop(\"pynq_deploy_dir\")\n",
    "    # extracting last folder of absolute path (deployment_dir)\n",
    "    deployment_folder = os.path.basename(os.path.normpath(deployment_dir))\n",
    "    platform = model.get_metadata_prop(\"platform\")\n",
    "    assert platform in [\"alveo\", \"zynq-iodma\"]\n",
    "    bitfile = model.get_metadata_prop(\"bitfile\")\n",
    "    bitfile = os.path.basename(bitfile)\n",
    "    if pynq_password == \"\":\n",
    "        if \"zynq\" in platform:\n",
    "            raise Exception(\"PYNQ board remote exec needs password for sudo\")\n",
    "        else:\n",
    "            local_prefix = \"\"  # assume we are using an ssh key\n",
    "            warnings.warn(\"Empty password, make sure you've set up an ssh key\")\n",
    "    else:\n",
    "        local_prefix = \"sshpass -p %s \" % pynq_password\n",
    "\n",
    "    if platform == \"alveo\":\n",
    "        # Alveo can run without sudo but needs correct environment\n",
    "        remote_prefix = \"conda activate finn-pynq-alveo; \"\n",
    "    elif \"zynq\" in platform:\n",
    "        # PYNQ Zynq boards need to execute with sudo\n",
    "        remote_prefix = \"echo %s | sudo -S \" % pynq_password\n",
    "\n",
    "    # use platform attribute for correct remote execution\n",
    "    if platform == \"alveo\":\n",
    "        remote_cmd = \"bash -ic 'bash alveo_run.sh throughput_test %d' \\\"\" % batchsize\n",
    "    else:\n",
    "        remote_cmd = (\n",
    "            \"python3.6 driver.py --exec_mode=throughput_test --batchsize={} \"\n",
    "            \"--bitfile={} --inputfile=input.npy --outputfile=output.npy \"\n",
    "            '--platform={} \"'\n",
    "        ).format(batchsize, bitfile, platform)\n",
    "    cmd = (\n",
    "        local_prefix + 'ssh {}@{} -p {} \"cd {}/{}; ' + remote_prefix + remote_cmd\n",
    "    ).format(pynq_username, pynq_ip, pynq_port, pynq_target_dir, deployment_folder)\n",
    "    bash_command = [\"/bin/bash\", \"-c\", cmd]\n",
    "    process_throughput_test = subprocess.Popen(bash_command, stdout=subprocess.PIPE)\n",
    "    process_throughput_test.communicate(timeout=timeout)\n",
    "\n",
    "    # remove any pre-existing metrics file\n",
    "    try:\n",
    "        os.remove(\"{}/nw_metrics.txt\".format(deployment_dir))\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "    cmd = local_prefix + \"scp -P{} {}@{}:{}/{}/nw_metrics.txt {}\".format(\n",
    "        pynq_port,\n",
    "        pynq_username,\n",
    "        pynq_ip,\n",
    "        pynq_target_dir,\n",
    "        deployment_folder,\n",
    "        deployment_dir,\n",
    "    )\n",
    "    bash_command = [\"/bin/bash\", \"-c\", cmd]\n",
    "    process_compile = subprocess.Popen(bash_command, stdout=subprocess.PIPE)\n",
    "    process_compile.communicate(timeout=timeout)\n",
    "\n",
    "    try:\n",
    "        with open(\"{}/nw_metrics.txt\".format(deployment_dir), \"r\") as file:\n",
    "            res = eval(file.read())\n",
    "        return res\n",
    "    except FileNotFoundError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_checkpoint_name_generator(build_dir, save_file_prefix, topology, wbits, abits, parameter_priority):\n",
    "    priority_string = \"prio_\" + \"-\".join(parameter_priority)\n",
    "    def get_checkpoint_name(step, get_full_path=True):\n",
    "        \n",
    "        if get_full_path:\n",
    "            return build_dir + \"/end2end_%s_%s_%s_w%da%d_%s.onnx\" % (save_file_prefix, priority_string, topology, wbits, abits, step)\n",
    "        else:\n",
    "            if step == \"\":\n",
    "                return \"%s_%s_%s_w%da%d\" % (save_file_prefix, priority_string, topology, wbits, abits)\n",
    "            else:\n",
    "                return \"%s_%s_%s_w%da%d_%s\" % (save_file_prefix, priority_string, topology, wbits, abits, step)\n",
    "            \n",
    "    return get_checkpoint_name\n",
    "\n",
    "# custom functions\n",
    "def get_trained_network_and_ishape(topology, wbits, abits):\n",
    "    \"Return (trained_model, shape) for given BNN-PYNQ test config.\"\n",
    "\n",
    "    topology_to_ishape = {\n",
    "        \"tfc\": (1, 1, 28, 28),\n",
    "        \"cnv\": (1, 3, 32, 32),\n",
    "    }\n",
    "    ishape = topology_to_ishape[topology]\n",
    "    model = get_test_model_trained(topology.upper(), wbits, abits)\n",
    "    return (model, ishape)\n",
    "\n",
    "def get_test_model_trained(netname, wbits, abits):\n",
    "    \"get_test_model with pretrained=True\"\n",
    "    return get_test_model(netname, wbits, abits, pretrained=True)\n",
    "\n",
    "def get_test_model(netname, wbits, abits, pretrained):\n",
    "    \"\"\"Returns the model specified by input arguments from the Brevitas BNN-PYNQ\n",
    "    test networks. Pretrained weights loaded if pretrained is True.\"\"\"\n",
    "    if ((wbits > 2) or (abits > 2)) or ((wbits == 2) and (abits == 1)):\n",
    "        # get own custom trained models\n",
    "        if netname == \"CNV\":\n",
    "            from brevitas_examples.bnn_pynq.models.CNV import CNV\n",
    "            num_classes = 10\n",
    "            fc = CNV(num_classes, wbits, abits, 8, 3)\n",
    "            if pretrained:\n",
    "                # Load checkpoint from disk\n",
    "                checkpoint_folder_location = f\"../../CIFAR10_networks_only_tar_files/CIFAR{num_classes}_CNV_{wbits}W{abits}A_*\"\n",
    "                folder_list = glob(checkpoint_folder_location)\n",
    "                checkpoint_folder_location = folder_list[0] + \"/checkpoints/\"\n",
    "                print(\"loading from folder: \", checkpoint_folder_location)\n",
    "\n",
    "                checkpoint_dict = torch.load(checkpoint_folder_location + \"best.tar\", map_location=torch.device('cpu'))\n",
    "                checkpoint_dict.keys(), checkpoint_dict[\"epoch\"]\n",
    "\n",
    "                # load saved training results\n",
    "                fc.load_state_dict(checkpoint_dict[\"state_dict\"], strict=True)\n",
    "        else:\n",
    "            raise RuntimeError(\"Custom weights are only supported for the CNV architecture\")\n",
    "    else:\n",
    "        # get the models in the standard way\n",
    "        model_cfg = (netname, wbits, abits)\n",
    "        model_def_fxn = example_map[model_cfg]\n",
    "        fc = model_def_fxn(pretrained)\n",
    "    return fc.eval()\n",
    "\n",
    "def load_bit_model(get_checkpoint_name, weight_bit_width, act_bit_width):\n",
    "    # get model\n",
    "    (model, ishape) = get_trained_network_and_ishape(\"cnv\", weight_bit_width, act_bit_width)\n",
    "    \n",
    "    # export to onnx\n",
    "    bo.export_finn_onnx(model, ishape, get_checkpoint_name(\"export\"))\n",
    "    \n",
    "    # Hotfix for non-unity multiplilcation in front of MaxPool\n",
    "    model = ModelWrapper(get_checkpoint_name(\"export\"))\n",
    "    model = model.transform(GiveUniqueNodeNames())\n",
    "    model = model.transform(GiveReadableTensorNames())\n",
    "    \n",
    "    if ((weight_bit_width > 2) or (act_bit_width > 2)) or ((weight_bit_width == 2) and (act_bit_width == 1)):\n",
    "        # The Tensors, which need adjusting to 1 are: Mul_5_param0, Mul_9_param0\n",
    "        tensor_names_to_adjust = [\"Mul_5_param0\", \"Mul_9_param0\", \"Mul_3_param0\", \"Mul_7_param0\", \"Mul_11_param0\", \"Mul_13_param0\", \"Mul_15_param0\", \"Mul_17_param0\"]\n",
    "        # New for W2A1\n",
    "        tensor_names_to_adjust.extend([\"Mul_1_param0\"])\n",
    "        # Value to set to\n",
    "        target_value = 1.\n",
    "        # Check that these tensores actually exist and are of right shape\n",
    "        init_names = [x.name for x in model.graph.initializer if x.name in tensor_names_to_adjust]\n",
    "        for name in init_names:\n",
    "            assert model.get_initializer(name).size == 1, \"Multiplication is not a single value, but it should!\"\n",
    "            # Set tensor to target value, while retaining the datatype\n",
    "            init_arr = model.get_initializer(name) \n",
    "            target_dtype = init_arr.dtype\n",
    "            new_init_arr = np.array(target_value, dtype=target_dtype)\n",
    "            model.set_initializer(name, new_init_arr)\n",
    "    \n",
    "    model.save(get_checkpoint_name(\"export_2\"))\n",
    "\n",
    "    model = ModelWrapper(get_checkpoint_name(\"export_2\"))\n",
    "    #model = model.transform(DoubleToSingleFloat())\n",
    "    model = model.transform(InferShapes())\n",
    "    model = model.transform(FoldConstants())\n",
    "    model = model.transform(GiveUniqueNodeNames())\n",
    "    model = model.transform(GiveReadableTensorNames())\n",
    "    model = model.transform(InferDataTypes())\n",
    "    model = model.transform(RemoveStaticGraphInputs())\n",
    "    model.save(get_checkpoint_name(\"tidy\"))\n",
    "    return model\n",
    "    \n",
    "\n",
    "def add_pre_and_postproc(get_checkpoint_name):\n",
    "    prev_chkpt_name = get_checkpoint_name(\"tidy\")\n",
    "    model = ModelWrapper(prev_chkpt_name)\n",
    "    global_inp_name = model.graph.input[0].name\n",
    "    ishape = model.get_tensor_shape(global_inp_name)\n",
    "    # preprocessing: torchvision's ToTensor divides uint8 inputs by 255\n",
    "    totensor_pyt = ToTensor()\n",
    "    chkpt_preproc_name = get_checkpoint_name(\"preproc\")\n",
    "    bo.export_finn_onnx(totensor_pyt, ishape, chkpt_preproc_name)\n",
    "    assert os.path.isfile(chkpt_preproc_name)\n",
    "    # join preprocessing and core model\n",
    "    pre_model = ModelWrapper(chkpt_preproc_name)\n",
    "    model = model.transform(MergeONNXModels(pre_model))\n",
    "    # add input quantization annotation: UINT8 for all BNN-PYNQ models\n",
    "    global_inp_name = model.graph.input[0].name\n",
    "    model.set_tensor_datatype(global_inp_name, DataType.UINT8)\n",
    "    # postprocessing: insert Top-1 node at the end\n",
    "    model = model.transform(InsertTopK(k=1))\n",
    "    chkpt_name = get_checkpoint_name(\"pre_post\")\n",
    "    # tidy-up again\n",
    "    model = model.transform(InferShapes())\n",
    "    model = model.transform(FoldConstants())\n",
    "    model = model.transform(GiveUniqueNodeNames())\n",
    "    model = model.transform(GiveReadableTensorNames())\n",
    "    model = model.transform(InferDataTypes())\n",
    "    model = model.transform(RemoveStaticGraphInputs())\n",
    "    model.save(chkpt_name)\n",
    "    return model\n",
    "    \n",
    "def streamline_model(get_checkpoint_name):\n",
    "    # check if target file already exists\n",
    "    target_file = get_checkpoint_name(\"streamline\")\n",
    "    model = ModelWrapper(get_checkpoint_name(\"pre_post\"))\n",
    "    # move past any reshapes to be able to streamline input scaling\n",
    "    model = model.transform(MoveScalarLinearPastInvariants())\n",
    "    model = model.transform(Streamline())\n",
    "    # only doing cnv networks\n",
    "    #if \"fc\" not in topology:\n",
    "    model = model.transform(LowerConvsToMatMul())\n",
    "    model = model.transform(MakeMaxPoolNHWC())\n",
    "    model = model.transform(absorb.AbsorbTransposeIntoMultiThreshold())\n",
    "    \n",
    "    model = model.transform(ConvertBipolarMatMulToXnorPopcount())\n",
    "    model = model.transform(Streamline())\n",
    "    # absorb final add-mul nodes into TopK\n",
    "    model = model.transform(absorb.AbsorbScalarMulAddIntoTopK())\n",
    "    model = model.transform(InferDataLayouts())\n",
    "    model = model.transform(RemoveUnusedTensors())\n",
    "    model.save(target_file)\n",
    "    return model\n",
    "\n",
    "def convert_to_hls_layers(get_checkpoint_name, SWG_SIMD_list = None, pruning_ratio = None, pruning_mode = None, mem_mode = \"decoupled\"):    \n",
    "    prev_chkpt_name = get_checkpoint_name(\"streamline\")\n",
    "    model = ModelWrapper(prev_chkpt_name)\n",
    "    # needed for bipolar MatMul layers\n",
    "    model = model.transform(to_hls.InferBinaryStreamingFCLayer(mem_mode))\n",
    "    # needed for non-bipolar MatMul layers\n",
    "    model = model.transform(to_hls.InferQuantizedStreamingFCLayer(mem_mode))\n",
    "    # TopK to LabelSelect\n",
    "    model = model.transform(to_hls.InferLabelSelectLayer())\n",
    "    # input quantization (if any) to standalone thresholding\n",
    "    model = model.transform(to_hls.InferThresholdingLayer())\n",
    "    # needed for convolutions\n",
    "    # only doing cnv networks\n",
    "    #if \"fc\" not in topology:\n",
    "    # Introduce pruned layers\n",
    "    if pruning_ratio == None:\n",
    "        model = model.transform(to_hls.InferConvInpGen())\n",
    "    else:\n",
    "        if pruning_mode == \"coarse\":\n",
    "            model = model.transform(to_hls.InferConvInpGenPruned(pruning_ratio, adjust_following_MVAU=True, SIMD_list=SWG_SIMD_list))\n",
    "        elif pruning_mode == \"fine\":\n",
    "            model = model.transform(to_hls.InferConvInpGenSIMDPruned(SWG_SIMD_list, pruning_ratio, adjust_following_MVAU=True))\n",
    "        else:\n",
    "            raise ValueError(f\"Pruning mode {pruning_mode} not supported!\")\n",
    "    model = model.transform(to_hls.InferStreamingMaxPool())\n",
    "    model = model.transform(RemoveCNVtoFCFlatten())\n",
    "    \n",
    "    # get rid of Tranpose -> Tranpose identity seq\n",
    "    model = model.transform(absorb.AbsorbConsecutiveTransposes())\n",
    "    model = model.transform(GiveUniqueNodeNames())\n",
    "    model = model.transform(InferDataLayouts())\n",
    "    model.save(get_checkpoint_name(\"convert_to_hls_layers\"))\n",
    "    return model\n",
    "\n",
    "def creat_dataflow_model(get_checkpoint_name):\n",
    "    prev_chkpt_name = get_checkpoint_name(\"convert_to_hls_layers\")\n",
    "    model = ModelWrapper(prev_chkpt_name)\n",
    "    parent_model = model.transform(CreateDataflowPartition())\n",
    "    parent_model_chkpt = get_checkpoint_name(\"dataflow_parent\")\n",
    "    parent_model.save(parent_model_chkpt)\n",
    "    sdp_node = parent_model.get_nodes_by_op_type(\"StreamingDataflowPartition\")[0]\n",
    "    sdp_node = getCustomOp(sdp_node)\n",
    "    dataflow_model_filename = sdp_node.get_nodeattr(\"model\")\n",
    "    dataflow_model = load_test_checkpoint_or_skip(dataflow_model_filename)\n",
    "    dataflow_model_chkpt = get_checkpoint_name(\"dataflow_model\")\n",
    "    dataflow_model.save(dataflow_model_chkpt)\n",
    "    return dataflow_model\n",
    "\n",
    "def configure_folding(get_checkpoint_name, folding):\n",
    "    prev_chkpt_name = get_checkpoint_name(\"dataflow_model\")\n",
    "    model = ModelWrapper(prev_chkpt_name)\n",
    "    \n",
    "    model = model.transform(GiveUniqueNodeNames())\n",
    "    fc_layers = model.get_nodes_by_op_type(\"StreamingFCLayer_Batch\")\n",
    "    for fcl in fc_layers:\n",
    "        name = fcl.name\n",
    "        pe, simd, ififodepth, ramstyle = folding[name]\n",
    "        fcl_inst = getCustomOp(fcl)\n",
    "\n",
    "        # Check compatibility\n",
    "        mw = fcl_inst.get_nodeattr(\"MW\")\n",
    "        mh = fcl_inst.get_nodeattr(\"MH\")\n",
    "        if not (mh % pe == 0):\n",
    "            raise ValueError(f'FINN requirement \"MH divisable by PE\" is violated. For layer: {name}; with MH: {mh}, PE: {pe}')\n",
    "        if not (mw % simd == 0):\n",
    "            raise ValueError(f'FINN requirement \"MW divisable by SIMD\" is violated. For layer: {name}; with MW: {mw}, SIMD: {simd}')\n",
    "        if not (simd > (mw / 1024)):\n",
    "            raise ValueError(f'Vivado requirement \"SIMD > MW / 1024\" is violated. For layer: {name}; with MW: {mw}, SIMD: {simd}')\n",
    "\n",
    "        # Apply parameters\n",
    "        fcl_inst.set_nodeattr(\"PE\", pe)\n",
    "        fcl_inst.set_nodeattr(\"inFIFODepth\", ififodepth)\n",
    "        fcl_inst.set_nodeattr(\"ram_style\", ramstyle)\n",
    "        # Only adjust SIMD, if it wasn't in previously changed (by the pruning)\n",
    "        # Check that the privious input (im2col) was not a pruned layer\n",
    "        if not (\"Pruned\" in model.find_direct_predecessors(fcl)[0].name):\n",
    "            # Only apply SIMD, if the layer wasn't changed by the pruning layer\n",
    "            fcl_inst.set_nodeattr(\"SIMD\", simd)\n",
    "\n",
    "    # use same SIMD values for the sliding window operators\n",
    "    folding_list = list(folding.values())\n",
    "    swg_idepth = [2, 51, 9, 106, 2, 2]\n",
    "    swg_layer_name_list = [\"ConvolutionInputGenerator\", \"ConvolutionInputGeneratorPruned\", \"ConvolutionInputGeneratorSIMDPruned\"]\n",
    "    for swg_layer_name in swg_layer_name_list:\n",
    "        swg_layers = model.get_nodes_by_op_type(swg_layer_name)\n",
    "        for i in range(len(swg_layers)):\n",
    "            swg_inst = getCustomOp(swg_layers[i])\n",
    "            # Apply parameters            \n",
    "            # SIMD was already applied for the pruned swg nodes\n",
    "            if not (\"Pruned\" in swg_layer_name):\n",
    "                ifm_ch = swg_inst.get_nodeattr(\"IFMChannels\")\n",
    "                simd = folding_list[i][1]\n",
    "                # Check compatibility\n",
    "                if not (ifm_ch % simd == 0):\n",
    "                    raise ValueError(f'FINN requirement \"IFMChannels divisable by SIMD\" is violated. For layer: {name}; with IFMChannels: {ifm_ch}, SIMD: {simd}')\n",
    "                swg_inst.set_nodeattr(\"SIMD\", simd)\n",
    "            swg_inst.set_nodeattr(\"inFIFODepth\", swg_idepth[i])\n",
    "\n",
    "    model = model.transform(GiveUniqueNodeNames())\n",
    "    model.save(get_checkpoint_name(\"fold\"))\n",
    "    return model\n",
    "\n",
    "def build_for_hardware(get_checkpoint_name, test_pynq_board, target_clk_ns, auto_set_FIFO_depth):\n",
    "    prev_chkpt_name = get_checkpoint_name(\"fold\")\n",
    "    model = ModelWrapper(prev_chkpt_name)\n",
    "    model = model.transform(ZynqBuild(platform = test_pynq_board, \n",
    "                                      period_ns = target_clk_ns, \n",
    "                                      auto_set_FIFO_depth = auto_set_FIFO_depth\n",
    "                                     ))\n",
    "    model.save(get_checkpoint_name(\"synth\"))\n",
    "    return model\n",
    "\n",
    "def throughput_rtl_sim(get_checkpoint_name, test_pynq_board, target_clk_ns):\n",
    "    test_fpga_part = pynq_part_map[test_pynq_board]\n",
    "    \n",
    "    # test_ipgen\n",
    "    prev_chkpt_name = get_checkpoint_name(\"fold\")\n",
    "    model = ModelWrapper(prev_chkpt_name)\n",
    "    model = model.transform(GiveUniqueNodeNames())\n",
    "    model = model.transform(PrepareIP(test_fpga_part, target_clk_ns))\n",
    "    model = model.transform(HLSSynthIP())\n",
    "    model.save(get_checkpoint_name(\"ipgen_rtlsim\"))\n",
    "    \n",
    "    # test_ipstitch_rtlsim\n",
    "    prev_chkpt_name = get_checkpoint_name(\"ipgen_rtlsim\")\n",
    "    model = ModelWrapper(prev_chkpt_name)\n",
    "    model = model.transform(InsertDWC())\n",
    "    model = model.transform(InsertFIFO())\n",
    "    model = model.transform(GiveUniqueNodeNames())\n",
    "    model = model.transform(AnnotateCycles())\n",
    "    perf = model.analysis(dataflow_performance)\n",
    "    latency = perf[\"critical_path_cycles\"]\n",
    "    model = model.transform(PrepareIP(test_fpga_part, target_clk_ns))\n",
    "    model = model.transform(HLSSynthIP())\n",
    "    model = model.transform(CreateStitchedIP(test_fpga_part, target_clk_ns))\n",
    "    model = model.transform(PrepareRTLSim())\n",
    "    model.set_metadata_prop(\"exec_mode\", \"rtlsim\")\n",
    "    os.environ[\"LIVENESS_THRESHOLD\"] = str(int(latency * 1.1))\n",
    "    vcdf = get_checkpoint_name(\"vcd\") + \".vcd\"\n",
    "    model.set_metadata_prop(\"rtlsim_trace\", vcdf)\n",
    "    os.environ[\"RTLSIM_TRACE_DEPTH\"] = \"3\"\n",
    "    rtlsim_chkpt = get_checkpoint_name(\"ipstitch_rtlsim\")\n",
    "    model.save(rtlsim_chkpt)\n",
    "    parent_chkpt = get_checkpoint_name(\"dataflow_parent\")\n",
    "    input_tensor_npy = get_example_input(\"cnv\")\n",
    "    y = execute_parent(parent_chkpt, rtlsim_chkpt, input_tensor_npy)\n",
    "    model = ModelWrapper(rtlsim_chkpt)\n",
    "    perf[\"cycles_rtlsim\"] = model.get_metadata_prop(\"cycles_rtlsim\")\n",
    "    \n",
    "    # test_throughput_rtlsim\n",
    "    prev_chkpt_name = get_checkpoint_name(\"ipstitch_rtlsim\")\n",
    "    model = ModelWrapper(prev_chkpt_name)\n",
    "    n_nodes = len(model.graph.node)\n",
    "    perf_est = model.analysis(dataflow_performance)\n",
    "    latency = int(model.get_metadata_prop(\"cycles_rtlsim\"))\n",
    "    cycles_per_sample_est = perf_est[\"max_cycles\"]\n",
    "    batchsize = 2 * n_nodes\n",
    "    # limit maximum batchsize\n",
    "    if batchsize > 100:\n",
    "        batchsize = 100\n",
    "    ret = throughput_test_rtlsim(model, batchsize=batchsize)\n",
    "    res_cycles = ret[\"cycles\"]\n",
    "    est_cycles = latency + cycles_per_sample_est * batchsize\n",
    "    \n",
    "    rtl_throughput_dict = {\n",
    "        \"perf\":perf, \n",
    "        \"latency\":latency,\n",
    "        \"n_nodes\":n_nodes, \n",
    "        \"batchsize\": batchsize,\n",
    "        \"perf_est\":perf_est, \n",
    "        \"latency\":latency, \n",
    "        \"cycles_per_sample_est\":cycles_per_sample_est, \n",
    "        \"ret\":ret, \n",
    "        \"res_cycles\":res_cycles,\n",
    "        \"est_cycles\":est_cycles,\n",
    "    }\n",
    "    \n",
    "    return model, rtl_throughput_dict\n",
    "\n",
    "\n",
    "def extract_fifo_max(get_checkpoint_name):\n",
    "    vcdf = get_checkpoint_name(\"vcd\") + \".vcd\"\n",
    "    stream_ifs = vcd.list_stream_if(vcdf)\n",
    "    fifos = vcd.list_fifo_count_signals(vcdf)\n",
    "    fifo_max = vcd.get_all_fifo_count_max(vcdf)\n",
    "    return fifo_max\n",
    "\n",
    "def extract_stream_stats(get_checkpoint_name):\n",
    "    vcdf = get_checkpoint_name(\"vcd\") + \".vcd\"\n",
    "    stream_stats = get_all_stream_if_stats(vcdf)\n",
    "    return stream_stats\n",
    "\n",
    "def delete_vcd_file(get_checkpoint_name):\n",
    "    vcdf = get_checkpoint_name(\"vcd\") + \".vcd\"\n",
    "    if os.path.exists(vcdf):\n",
    "        os.remove(vcdf)\n",
    "\n",
    "\n",
    "def extract_synth_and_ppr_timing_resources(get_checkpoint_name):\n",
    "    # Get the paths to the different report files\n",
    "    model = ModelWrapper(get_checkpoint_name(\"synth\"))\n",
    "    vivado_proj = model.get_metadata_prop(\"vivado_pynq_proj\")\n",
    "    # const parameter\n",
    "    ppr_report_path = vivado_proj + \"/finn_zynq_link.runs/impl_1/top_wrapper_utilization_placed.rpt\"\n",
    "    synth_report_path = vivado_proj + \"/synth_report.xml\"\n",
    "    timing_report_path = vivado_proj + \"/finn_zynq_link.runs/impl_1/top_wrapper_timing_summary_routed.rpt\"\n",
    "    \n",
    "    # Read post place utilization report\n",
    "    with open(ppr_report_path) as f:\n",
    "        content = f.readlines()\n",
    "    \n",
    "    top_levels_to_parse = [\"1. Slice Logic\", \"2. Slice Logic Distribution\", \"3. Memory\", \"4. DSP\", \"6. Clocking\"]\n",
    "    new_top_levels = [\"1. CLB Logic\", \"2. CLB Logic Distribution\", \"3. BLOCKRAM\", \"4. ARITHMETIC\", \"6. CLOCK\"]\n",
    "    top_levels_to_parse.extend(new_top_levels)\n",
    "\n",
    "    utilization_data = {}\n",
    "    top_level_key = \"\"\n",
    "    parse_enable = False\n",
    "    waiting_for_table_start = False\n",
    "\n",
    "    for line in content:\n",
    "        # check if we stumbled upon one of the top level indicators\n",
    "        if any(top in line for top in top_levels_to_parse):\n",
    "            waiting_for_table_start = True\n",
    "            # Find out which of them it was\n",
    "            for i in range(len(top_levels_to_parse)):\n",
    "                if top_levels_to_parse[i] in line:\n",
    "                    top_level_key = top_levels_to_parse[i]\n",
    "                    break\n",
    "            utilization_data[top_level_key] = []\n",
    "\n",
    "        # Check for table start indicator\n",
    "        if waiting_for_table_start:\n",
    "            if \"Available\" in line:\n",
    "                parse_enable = True\n",
    "                waiting_for_table_start = False\n",
    "                continue\n",
    "\n",
    "        # parse a line\n",
    "        if parse_enable:\n",
    "            # reached a table border\n",
    "            if \"+--\" in line:\n",
    "                continue\n",
    "            # reached end of table\n",
    "            if \"\\n\" == line or \"* Note: Each Block RAM Tile only h\" in line:\n",
    "                parse_enable = False\n",
    "                continue\n",
    "            # parse table row\n",
    "            line = line.strip()\n",
    "            split_line = line.split(\"|\")[1:-1]\n",
    "            row_data = []\n",
    "            for data_snipplet in split_line:\n",
    "                d = data_snipplet.strip()\n",
    "                try:\n",
    "                    d = float(d)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "                row_data.append(d)\n",
    "            # skip rows with emtpy elements\n",
    "            if '' in row_data:\n",
    "                continue\n",
    "            utilization_data[top_level_key].append(row_data)\n",
    "    \n",
    "    # Read the synthesis report\n",
    "    root = ET.parse(synth_report_path).getroot()\n",
    "\n",
    "    # Read the table header of the synthesis report\n",
    "    table_header = []\n",
    "    for child in root[0][0][0]:\n",
    "        attribs = child.attrib\n",
    "        table_header.append(attribs['contents'])\n",
    "\n",
    "    # Read the rest of the table\n",
    "    table_cols = []\n",
    "    for child1 in root[0][0][1:]:\n",
    "        one_col = []\n",
    "        for child2 in child1:\n",
    "            content = child2.attrib['contents'].strip()\n",
    "            # Everything that looks like an int should become an int\n",
    "            try:\n",
    "                content = int(content)\n",
    "            except ValueError:\n",
    "                pass\n",
    "            one_col.append(content)\n",
    "        table_cols.append(one_col)\n",
    "\n",
    "\n",
    "    # Store data in a nicer format\n",
    "    report_data = pd.DataFrame(table_cols, columns=table_header)\n",
    "    \n",
    "    # Get timing data\n",
    "\n",
    "    timing_data = {}\n",
    "\n",
    "    with open(timing_report_path) as f:\n",
    "        content = f.readlines()\n",
    "\n",
    "    found_start = False\n",
    "    lines_to_header = 5\n",
    "    lines_to_content = 7\n",
    "    data_title = \"\"\n",
    "\n",
    "    line_count = 0\n",
    "    for line in content:\n",
    "        if (\"Design Timing Summary\" in line) or (\"Clock Summary\" in line):\n",
    "            found_start = True\n",
    "            data_title = line[1:].strip()\n",
    "        if found_start:\n",
    "            line_count += 1\n",
    "        if line_count == lines_to_header:\n",
    "            headers = line.strip().split(\"  \")\n",
    "            headers = list(filter(lambda x: not (x == \"\"), headers ))\n",
    "        if line_count == lines_to_content:\n",
    "            data_part = line.strip().split(\"  \")\n",
    "            data_part = list(filter(lambda x: not (x == \"\"), data_part))\n",
    "            #data_part = [float(x) for x in data_part]\n",
    "\n",
    "            # reset start finder\n",
    "            found_start =  False\n",
    "            line_count = 0\n",
    "\n",
    "            # Save results\n",
    "            res_dict = {}\n",
    "            for i in range(len(headers)):\n",
    "                try:\n",
    "                    res_dict[headers[i]] = float(data_part[i])\n",
    "                except ValueError:\n",
    "                    res_dict[headers[i]] = data_part[i]\n",
    "            timing_data[data_title] = res_dict\n",
    "    \n",
    "    return utilization_data, report_data, timing_data\n",
    "\n",
    "\n",
    "def run_throughput_test_on_HW(get_checkpoint_name, HW_lock, ip = os.getenv(\"PYNQ_IP\", \"192.168.1.106\"),\n",
    "                                username = os.getenv(\"PYNQ_USERNAME\", \"xilinx\"),\n",
    "                                password = os.getenv(\"PYNQ_PASSWORD\", \"xilinx\"),\n",
    "                                port = os.getenv(\"PYNQ_PORT\", 22),\n",
    "                                target_dir = os.getenv(\"PYNQ_TARGET_DIR\", \"/home/xilinx/finn\"),\n",
    "                                timeout=None,\n",
    "                             ):\n",
    "    ret_str, ret = \"\", {}\n",
    "    largest_bsize = 0\n",
    "    # Acquire the HW lock first, so that we make sure to only run one throughput test at a time\n",
    "    with HW_lock:\n",
    "        # Deploy to FPGA board\n",
    "        # test_deploy\n",
    "        model = ModelWrapper(get_checkpoint_name(\"synth\"))\n",
    "        model = model.transform(\n",
    "            DeployToPYNQ(\n",
    "                ip,\n",
    "                port,\n",
    "                username,\n",
    "                password,\n",
    "                target_dir,\n",
    "                timeout=timeout,\n",
    "            )\n",
    "        )\n",
    "        # save the model to be able to link it to the parent\n",
    "        model.save(get_checkpoint_name(\"deploy\"))\n",
    "        \n",
    "        # test_throughput_hw\n",
    "        test_name = get_checkpoint_name(\"\", get_full_path=False)\n",
    "        model = ModelWrapper(get_checkpoint_name(\"deploy\"))\n",
    "        ret = dict()\n",
    "        # try a range of batch sizes, some may fail due to insufficient DMA\n",
    "        # buffers\n",
    "        bsize_range_in = [8 ** i for i in range(5)]\n",
    "        bsize_range = []\n",
    "        largest_bsize = None\n",
    "        for bsize in bsize_range_in:\n",
    "            res = throughput_test_remote(model, bsize, timeout=timeout)\n",
    "            #res = throughput_test_remote(model, bsize)\n",
    "            if res is not None:\n",
    "                ret[bsize] = res\n",
    "                bsize_range.append(bsize)\n",
    "                largest_bsize = bsize\n",
    "            else:\n",
    "                # assume we reached largest possible N\n",
    "                break\n",
    "    \n",
    "    HW_throughput_dict = {\n",
    "        \"ret\":ret,\n",
    "        \"largest_bsize\":largest_bsize,\n",
    "    }\n",
    "    return HW_throughput_dict\n",
    "\n",
    "def delete_onnx_and_finn_artifacts(build_dir, print_name):\n",
    "    left_onnx_files = glob(build_dir + f\"/*{print_name}*.onnx\")\n",
    "    node_attr_to_check = [\"code_gen_dir_ipgen\", \"rtlsim_so\", \"model\"]\n",
    "    model_attr_to_check = [\"pynq_driver_dir\", \"vivado_pynq_proj\", \"vivado_stitch_proj\", \n",
    "                           \"rtlsim_so\", \"pynq_deploy_dir\"]\n",
    "\n",
    "    new_onnx_models_to_check = []\n",
    "    last_folders_to_del = []\n",
    "\n",
    "    for path_list in [left_onnx_files, new_onnx_models_to_check]:\n",
    "        for path in path_list:\n",
    "            try:\n",
    "                curr_model = ModelWrapper(path)\n",
    "\n",
    "                folders_to_del = []\n",
    "\n",
    "                # Check out model\n",
    "                for attr in model_attr_to_check:\n",
    "                    #try:\n",
    "                    res = curr_model.get_metadata_prop(attr)\n",
    "                    if not res == None:\n",
    "                        if attr == \"rtlsim_so\":\n",
    "                                res = res.split(\"/\")\n",
    "                                res = \"/\".join(res[:-1])\n",
    "                        folders_to_del.append(res)\n",
    "                    #except AttributeError:\n",
    "                    #    pass\n",
    "\n",
    "                # Check out nodes\n",
    "                for node in curr_model.get_finn_nodes():\n",
    "                    n = getCustomOp(node)\n",
    "                    for attr in node_attr_to_check:\n",
    "                        try:\n",
    "                            res = n.get_nodeattr(attr)\n",
    "                            if attr == \"rtlsim_so\":\n",
    "                                res = res.split(\"/\")\n",
    "                                res = \"/\".join(res[:-1])\n",
    "                            if attr == \"model\":\n",
    "                                new_onnx_models_to_check.append(res)\n",
    "                                res = res.split(\"/\")\n",
    "                                res = \"/\".join(res[:-1])\n",
    "                                last_folders_to_del.append(res)\n",
    "                                break\n",
    "                            folders_to_del.append(res)\n",
    "                        except AttributeError:\n",
    "                            pass\n",
    "\n",
    "                # delete found folders\n",
    "                for dirpath in folders_to_del:\n",
    "                    if os.path.exists(dirpath) and os.path.isdir(dirpath):\n",
    "                        shutil.rmtree(dirpath)\n",
    "\n",
    "                # delete onnx file\n",
    "                if os.path.exists(path):\n",
    "                    os.remove(path)\n",
    "            except AssertionError:\n",
    "                # this happens when the model was added twice to the list and was already deleted, this happens sometimes\n",
    "                pass\n",
    "\n",
    "    # Do the final delete pass\n",
    "    for dirpath in last_folders_to_del:\n",
    "        if os.path.exists(dirpath) and os.path.isdir(dirpath):\n",
    "            shutil.rmtree(dirpath)\n",
    "\n",
    "\n",
    "def set_folding_for_model(folding, get_checkpoint_name, pruning_ratio = None, pruning_mode = None):\n",
    "    # We need to generate the model almost from scratch, due to the SIMD parameter having an impact on the insertion of the pruned layers\n",
    "    folding_list = list(folding.values())\n",
    "    SWG_SIMD_list = [fold[1] for fold in folding_list]\n",
    "    model = convert_to_hls_layers(get_checkpoint_name, SWG_SIMD_list = SWG_SIMD_list, pruning_ratio = pruning_ratio, pruning_mode = pruning_mode)\n",
    "    model = creat_dataflow_model(get_checkpoint_name)\n",
    "    model = configure_folding(get_checkpoint_name, folding)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def speed_up_layer(model, layer_name, folding, get_checkpoint_name, speedup_parameter=\"SIMD\", pruning_ratio = None, pruning_mode = None):\n",
    "    \n",
    "    if speedup_parameter == \"SIMD\":\n",
    "        paramter_index = 1\n",
    "    elif speedup_parameter == \"PE\":\n",
    "        paramter_index = 0\n",
    "    else:\n",
    "        raise ValueError(\"Can't optimize for paramter called \" + speedup_parameter)\n",
    "    \n",
    "    speed_up_succesfull = True\n",
    "    current_folding = copy.deepcopy(folding)\n",
    "    adjusted_layer_folding = current_folding[layer_name]\n",
    "    adjusted_layer_folding[paramter_index] *= 2\n",
    "    current_folding[layer_name] = adjusted_layer_folding\n",
    "    # try to apply the new folding\n",
    "    try:\n",
    "        model = set_folding_for_model(current_folding, get_checkpoint_name, pruning_ratio=pruning_ratio, pruning_mode=pruning_mode)\n",
    "    except ValueError as e:\n",
    "        speed_up_succesfull = False\n",
    "        return speed_up_succesfull, folding\n",
    "    return speed_up_succesfull, current_folding\n",
    "\n",
    "def autotune_folding(inital_folding, get_checkpoint_name, chip_LUTs = 70560, max_LUT_utilization_ratio = 1.2,\n",
    "                     parameter_priority = [\"balanced\"],\n",
    "                     improve_non_critical_layers = False,\n",
    "                     maximum_iterations = 200,\n",
    "                     verbose=False,\n",
    "                     pruning_ratio = None, pruning_mode = None\n",
    "                    ):\n",
    "    # Initilize variables\n",
    "    current_LUTs = 0.\n",
    "    last_folding = copy.deepcopy(inital_folding)\n",
    "    current_folding = copy.deepcopy(inital_folding)\n",
    "\n",
    "    i = 0\n",
    "    optimization_running = True\n",
    "    maximum_folding_improvement = False\n",
    "\n",
    "    # Start optimizing\n",
    "    while((current_LUTs/chip_LUTs < max_LUT_utilization_ratio) and optimization_running):\n",
    "        # when we arrive back here then the current folding becomes the last folding\n",
    "        last_folding = copy.deepcopy(current_folding)\n",
    "\n",
    "        # apply the last folding\n",
    "        model = set_folding_for_model(last_folding, get_checkpoint_name, pruning_ratio = pruning_ratio, pruning_mode = pruning_mode)\n",
    "        # calculate cycle statistics for the model\n",
    "        cycles_dict = exp_cycles_per_layer(model)\n",
    "        # Sort layers by latency\n",
    "        cycles_sorted = dict(sorted(cycles_dict.items(), key=lambda item: item[1]))\n",
    "\n",
    "        # select layer to improve \n",
    "        layer_index = -1\n",
    "        while optimization_running:\n",
    "            try:\n",
    "                slowest_layer = list(cycles_sorted.keys())[layer_index]\n",
    "            except IndexError:\n",
    "                if verbose:\n",
    "                    print(\"No more layers to optimize, ending optimization\")\n",
    "                optimization_running = False\n",
    "                maximum_folding_improvement = True\n",
    "                break\n",
    "            slowest_cycles = cycles_sorted[slowest_layer]\n",
    "            if verbose:\n",
    "                print(\"\\tImproving layer: \", slowest_layer)\n",
    "            # make the layer faster\n",
    "            layer_not_optimizable = False\n",
    "            if \"ConvolutionInputGenerator\" in slowest_layer:\n",
    "                # ConvolutionInputGenerators can only be tuned via the SIMD parameter\n",
    "                # So we disregard the PE parameter here and only try to improve the SIMD parameter\n",
    "                layer_name = slowest_layer.split(\"_\")[1]\n",
    "                layer_name = \"StreamingFCLayer_Batch_\" + layer_name\n",
    "                speed_up_succesfull, current_folding = speed_up_layer(model, layer_name, current_folding, get_checkpoint_name, speedup_parameter=\"SIMD\", pruning_ratio=pruning_ratio, pruning_mode=pruning_mode)\n",
    "                if not speed_up_succesfull:\n",
    "                    layer_not_optimizable = True\n",
    "            elif \"StreamingFCLayer_Batch\" in slowest_layer:\n",
    "                # This is for the normal StreamingFCLayer_Batch layers\n",
    "                if parameter_priority[0] == \"balanced\":\n",
    "                    # Balnced will try to adjust SIMD and PE to be equal\n",
    "                    # But SIMD will get increased before PE if both are already equal\n",
    "                    adjusted_layer_folding = current_folding[slowest_layer]\n",
    "                    if adjusted_layer_folding[1] > adjusted_layer_folding[0]:\n",
    "                        p_priority = [\"PE\", \"SIMD\"]\n",
    "                    else:\n",
    "                        p_priority = [\"SIMD\", \"PE\"]\n",
    "                else:\n",
    "                    p_priority = parameter_priority\n",
    "                for parameter in p_priority:\n",
    "                    speed_up_succesfull, current_folding = speed_up_layer(model, slowest_layer, current_folding, get_checkpoint_name, speedup_parameter=parameter, pruning_ratio=pruning_ratio, pruning_mode=pruning_mode)\n",
    "                    if speed_up_succesfull:\n",
    "                        break\n",
    "                if not speed_up_succesfull:\n",
    "                    layer_not_optimizable = True\n",
    "            else:\n",
    "                # Layer not recongnized\n",
    "                if improve_non_critical_layers:\n",
    "                    if verbose:\n",
    "                        print(\"Layer not recognized: \", slowest_layer, \" trying next\")\n",
    "                    layer_index -= 1\n",
    "                    continue\n",
    "                else:\n",
    "                    if verbose:\n",
    "                        print(\"Layer not recognized: \", slowest_layer, \" ending optimization\")\n",
    "                    optimization_running = False\n",
    "                    break\n",
    "\n",
    "            if layer_not_optimizable:\n",
    "                if improve_non_critical_layers:\n",
    "                    if verbose:\n",
    "                        print(\"No further improvement available for: \", slowest_layer, \" trying to improve the next best one\")\n",
    "                    layer_index -= 1\n",
    "                    continue\n",
    "                else:\n",
    "                    if verbose:\n",
    "                        print(\"No further improvement available for: \", slowest_layer, \" ending optimization\")\n",
    "                    optimization_running = False\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # try to apply the new folding\n",
    "        model = set_folding_for_model(current_folding, get_checkpoint_name, pruning_ratio = pruning_ratio, pruning_mode = pruning_mode)\n",
    "\n",
    "        # Calculate the resource utilization for the current model\n",
    "        res_dict = res_estimation(model)\n",
    "        current_LUTs = sum([v['LUT'] for v in res_dict.values()])\n",
    "        if verbose:\n",
    "            print(f\"current_LUTs={current_LUTs} of ultra96_maxLUT={chip_LUTs:.0f} \\t-> percent: {100*current_LUTs/chip_LUTs:.1f} [%]\")\n",
    "\n",
    "        i += 1\n",
    "        if i >= maximum_iterations:\n",
    "            if verbose:\n",
    "                print(\"Reached maximum number of iterations, ending optimization\")\n",
    "            break\n",
    "\n",
    "    # When the loop ends, then the last optimization overshot the resoruce budged\n",
    "    # This means that we can save the \"last_folding\" as the final result\n",
    "    if verbose:\n",
    "        print(f\"Optimization ended after {i} iterations\")\n",
    "    model = set_folding_for_model(last_folding, get_checkpoint_name, pruning_ratio = pruning_ratio, pruning_mode = pruning_mode)\n",
    "    return model, last_folding, maximum_folding_improvement\n",
    "\n",
    "def assemble_inital_folding(pruning_ratio, pruning_mode):\n",
    "    # find the inital folding depending on the pruning mode and ratio\n",
    "    if pruning_mode == None:\n",
    "        inital_folding_none = {\n",
    "            \"StreamingFCLayer_Batch_0\": [1, 3, 256, \"auto\"],\n",
    "            \"StreamingFCLayer_Batch_1\": [1, 1, 256, \"auto\"],\n",
    "            \"StreamingFCLayer_Batch_2\": [1, 1, 256, \"auto\"],\n",
    "            \"StreamingFCLayer_Batch_3\": [1, 2, 256, \"block\"],\n",
    "            \"StreamingFCLayer_Batch_4\": [1, 2, 214, \"auto\"],\n",
    "            \"StreamingFCLayer_Batch_5\": [1, 4, 2, \"auto\"],\n",
    "            \"StreamingFCLayer_Batch_6\": [1, 1, 126, \"distributed\"],\n",
    "            \"StreamingFCLayer_Batch_7\": [1, 1, 62, \"block\"],\n",
    "            \"StreamingFCLayer_Batch_8\": [5, 1, 6, \"distributed\"],\n",
    "        }\n",
    "        inital_folding = inital_folding_none\n",
    "    elif pruning_mode == \"coarse\":\n",
    "        inital_folding_coarse = {\n",
    "            \"StreamingFCLayer_Batch_0\": [1, 3, 256, \"auto\"],\n",
    "            \"StreamingFCLayer_Batch_1\": [1, 1, 256, \"auto\"],\n",
    "            \"StreamingFCLayer_Batch_2\": [1, 1, 256, \"auto\"],\n",
    "            \"StreamingFCLayer_Batch_3\": [1, 1, 256, \"block\"],\n",
    "            \"StreamingFCLayer_Batch_4\": [1, 1, 214, \"auto\"],\n",
    "            \"StreamingFCLayer_Batch_5\": [1, 4, 2, \"auto\"],\n",
    "            \"StreamingFCLayer_Batch_6\": [1, 1, 126, \"distributed\"],\n",
    "            \"StreamingFCLayer_Batch_7\": [1, 1, 62, \"block\"],\n",
    "            \"StreamingFCLayer_Batch_8\": [5, 1, 6, \"distributed\"],\n",
    "        }\n",
    "        inital_folding = inital_folding_coarse\n",
    "    elif pruning_mode == \"fine\":\n",
    "        #supported_p_r = [1-(1/(2**i)) for i in range(1,4)]\n",
    "        supported_p_r = [0.5, 0.75, 0.875]\n",
    "        if pruning_ratio == 0.5:\n",
    "            SIMD = 2\n",
    "        elif pruning_ratio == 0.75:\n",
    "            SIMD = 4\n",
    "        elif pruning_ratio == 0.875:\n",
    "            SIMD = 8\n",
    "        else:\n",
    "            raise ValueError(f\"Pruning ratio {pruning_ratio} not supported! Only: {supported_p_r} are supported for the fine method at this time.\")\n",
    "        inital_folding_fine = {\n",
    "            \"StreamingFCLayer_Batch_0\": [1, 3, 256, \"auto\"],\n",
    "            \"StreamingFCLayer_Batch_1\": [1, SIMD, 256, \"auto\"],\n",
    "            \"StreamingFCLayer_Batch_2\": [1, SIMD, 256, \"auto\"],\n",
    "            \"StreamingFCLayer_Batch_3\": [1, SIMD, 256, \"block\"],\n",
    "            \"StreamingFCLayer_Batch_4\": [1, SIMD, 214, \"auto\"],\n",
    "            \"StreamingFCLayer_Batch_5\": [1, SIMD, 2, \"auto\"],\n",
    "            \"StreamingFCLayer_Batch_6\": [1, 1, 126, \"distributed\"],\n",
    "            \"StreamingFCLayer_Batch_7\": [1, 1, 62, \"block\"],\n",
    "            \"StreamingFCLayer_Batch_8\": [5, 1, 6, \"distributed\"],\n",
    "        }\n",
    "        inital_folding = inital_folding_fine\n",
    "    else:\n",
    "        raise ValueError(f\"Pruning mode {pruning_mode} not supported!\")    \n",
    "    \n",
    "    return inital_folding\n",
    "\n",
    "def get_dataflow_model_strings(get_checkpoint_name):\n",
    "    dataflow_model_strings = []\n",
    "    prev_chkpt_name = get_checkpoint_name(\"synth\")\n",
    "    synth_model = ModelWrapper(prev_chkpt_name)\n",
    "    for node in synth_model.get_nodes_by_op_type(\"StreamingDataflowPartition\"):\n",
    "        node_inst = getCustomOp(node)\n",
    "        df_model = ModelWrapper(node_inst.get_nodeattr('model'))\n",
    "        dataflow_model_strings.append(str(df_model.model))\n",
    "    return dataflow_model_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_test(pruning_ratio, pruning_mode, wbits, abits, target_clk_ns, \n",
    "                 test_pynq_board, HW_lock, build_dir, topology, \n",
    "                 timeout=120, max_LUT_utilization_ratio_start = 1.4, parameter_priority = [\"SIMD\", \"PE\"],\n",
    "                 auto_set_FIFO_depth = False,\n",
    "                 start_export_data = None,\n",
    "                ):\n",
    "    save_file_prefix = generate_save_file_prefix(pruning_ratio, pruning_mode, target_clk_ns, auto_set_FIFO_depth)\n",
    "    get_checkpoint_name = get_checkpoint_name_generator(build_dir, save_file_prefix, topology, wbits, abits, parameter_priority)\n",
    "    print_name = get_checkpoint_name(\"\", get_full_path=False)\n",
    "    num_workers = read_num_workers_from_disk()\n",
    "    os.environ['NUM_DEFAULT_WORKERS'] = str(num_workers)\n",
    "\n",
    "    msg = \"Started test for: {}, and num_workers: {}\"\n",
    "    msg = msg.format(print_name, num_workers)\n",
    "    print(msg)\n",
    "\n",
    "    # Get folding to start with\n",
    "    inital_folding = assemble_inital_folding(pruning_ratio, pruning_mode)\n",
    "    \n",
    "    max_LUT_utilization_ratio = max_LUT_utilization_ratio_start\n",
    "    running = True\n",
    "    first_run = True\n",
    "    first_run_succeded = True\n",
    "    if not (start_export_data == None):\n",
    "        export_data = copy.deepcopy(start_export_data)\n",
    "    else:\n",
    "        export_data = {\n",
    "            \"largest PPRing max_LUT\": 0.0,\n",
    "            'PPR optimization successfull': False,\n",
    "            'last max_LUT for resuming': 0.,\n",
    "        }\n",
    "    \n",
    "    while(running):\n",
    "        # check if the run failed over all\n",
    "        if max_LUT_utilization_ratio <= 0.25:\n",
    "            running = False\n",
    "            print(f\"\\t{print_name}: PPR failed too many times, ending here.\")\n",
    "            break\n",
    "        # Initalize vars\n",
    "        no_synth_error = True\n",
    "        # run test\n",
    "        print(f\"\\t{print_name}: load_bit_model\")\n",
    "        model = load_bit_model(get_checkpoint_name, wbits, abits)\n",
    "        print(f\"\\t{print_name}: add_pre_and_postproc\")\n",
    "        model = add_pre_and_postproc(get_checkpoint_name)\n",
    "        print(f\"\\t{print_name}: streamline_model\")\n",
    "        model = streamline_model(get_checkpoint_name)\n",
    "        print(f\"\\t{print_name}: autotune_folding with max_LUT_utilization_ratio={max_LUT_utilization_ratio:.2f}\")\n",
    "        model, final_folding, maximum_folding_improvement = autotune_folding(inital_folding, get_checkpoint_name,\n",
    "                                                max_LUT_utilization_ratio = max_LUT_utilization_ratio,\n",
    "                                                parameter_priority = parameter_priority,\n",
    "                                                improve_non_critical_layers = True,\n",
    "                                                maximum_iterations = 200,\n",
    "                                                verbose = False,\n",
    "                                                pruning_ratio = pruning_ratio, pruning_mode = pruning_mode\n",
    "                                               )\n",
    "        print(f\"\\t{print_name}: {final_folding}\")\n",
    "\n",
    "        print(f\"\\t{print_name}: configure_folding\")\n",
    "        model = configure_folding(get_checkpoint_name, final_folding)\n",
    "        print(f\"\\t{print_name}: build_for_hardware\")\n",
    "        # Get num workers again, to make sure we are up to date\n",
    "        num_workers = read_num_workers_from_disk()\n",
    "        os.environ['NUM_DEFAULT_WORKERS'] = str(num_workers)\n",
    "        print(f\"\\t{print_name}: set env num workers to: {os.environ['NUM_DEFAULT_WORKERS']}\")\n",
    "        \n",
    "        build_error = None\n",
    "        ppr_report_error = None\n",
    "        rtl_sim_error = None\n",
    "        HW_test_error = None\n",
    "        dataflow_model_strings = None\n",
    "        try:\n",
    "            model = build_for_hardware(get_checkpoint_name,\n",
    "                                       test_pynq_board,\n",
    "                                       target_clk_ns,\n",
    "                                       auto_set_FIFO_depth,\n",
    "                                      )\n",
    "        except (Exception, AssertionError) as e:\n",
    "            print(f\"\\t{print_name}: No dice with synthesis, skipping to the end\")\n",
    "            print(e)\n",
    "            build_error = repr(e)\n",
    "            try:\n",
    "                print(f\"\\t{print_name}: extract_synth_and_ppr_timing_resources\")\n",
    "                utilization_data, report_data, timing_data = extract_synth_and_ppr_timing_resources(get_checkpoint_name)\n",
    "                report_data = report_data.to_json()\n",
    "            except (FileNotFoundError, AssertionError) as e:\n",
    "                print(f\"\\t{print_name}: No dice with synthesis, skipping\")\n",
    "                print(e)\n",
    "                ppr_report_error = repr(e)\n",
    "                utilization_data = None\n",
    "                report_data = None\n",
    "                timing_data = None\n",
    "                HW_throughput_dict = None\n",
    "            rtl_throughput_dict = None\n",
    "            fifo_max = None\n",
    "            stream_stats = None\n",
    "            HW_throughput_dict = None\n",
    "            # make sure to skip the rest\n",
    "            no_synth_error = False\n",
    "        \n",
    "        if no_synth_error:\n",
    "            try:\n",
    "                print(f\"\\t{print_name}: Skipping throughput_rtl_sim\")\n",
    "                rtl_throughput_dict = None\n",
    "                fifo_max = None\n",
    "                stream_stats = None\n",
    "                #print(f\"\\t{print_name}: throughput_rtl_sim\")\n",
    "                # Get num workers again, to make sure we are up to date\n",
    "                #num_workers = read_num_workers_from_disk()\n",
    "                #os.environ['NUM_DEFAULT_WORKERS'] = str(num_workers)\n",
    "                #model, rtl_throughput_dict = throughput_rtl_sim(get_checkpoint_name, test_pynq_board, target_clk_ns)\n",
    "                # Get num workers again, to make sure we are up to date\n",
    "                #num_workers = read_num_workers_from_disk()\n",
    "                #os.environ['NUM_DEFAULT_WORKERS'] = str(num_workers)\n",
    "                #print(f\"\\t{print_name}: extract_fifo_max\")\n",
    "                #fifo_max = extract_fifo_max(get_checkpoint_name)\n",
    "                #print(f\"\\t{print_name}: extract_stream_stats\")\n",
    "                #stream_stats = extract_stream_stats(get_checkpoint_name)\n",
    "                #print(f\"\\t{print_name}: delete_vcd_file\")\n",
    "                #delete_vcd_file(get_checkpoint_name)\n",
    "            except FileNotFoundError as e:\n",
    "                print(f\"\\t{print_name}: No dice with rtl sim, skipping\")\n",
    "                print(e)\n",
    "                rtl_sim_error = repr(e)\n",
    "                rtl_throughput_dict = None\n",
    "                fifo_max = None\n",
    "                stream_stats = None\n",
    "            try:\n",
    "                print(f\"\\t{print_name}: extract_synth_and_ppr_timing_resources\")\n",
    "                utilization_data, report_data, timing_data = extract_synth_and_ppr_timing_resources(get_checkpoint_name)\n",
    "                report_data = report_data.to_json()\n",
    "                print(f\"\\t{print_name}: get_dataflow_model_strings\")\n",
    "                dataflow_model_strings = get_dataflow_model_strings(get_checkpoint_name)\n",
    "                try:\n",
    "                    print(f\"\\t{print_name}: run_throughput_test_on_HW\")\n",
    "                    HW_throughput_dict = run_throughput_test_on_HW(get_checkpoint_name, HW_lock,\n",
    "                                                                   ip=\"ultra96-v2.ziti.uni-heidelberg.de\",\n",
    "                                                                   timeout=timeout,\n",
    "                                                                  )\n",
    "                except TimeoutExpired as e:\n",
    "                    print(f\"\\t{print_name}: HW test timed out\")\n",
    "                    HW_test_error = repr(e)\n",
    "                    HW_throughput_dict = None\n",
    "            except (FileNotFoundError, AssertionError) as e:\n",
    "                print(f\"\\t{print_name}: No dice with synthesis, skipping\")\n",
    "                print(e)\n",
    "                ppr_report_error = repr(e)\n",
    "                utilization_data = None\n",
    "                report_data = None\n",
    "                timing_data = None\n",
    "                HW_throughput_dict = None\n",
    "\n",
    "\n",
    "        # Do final clean up\n",
    "        print(f\"\\t{print_name}: delete_onnx_and_finn_artifacts\")\n",
    "        delete_onnx_and_finn_artifacts(build_dir, print_name)\n",
    "\n",
    "        # save data to disk\n",
    "        data = {\n",
    "            \"FINN version\": \"0.4b-dev\",\n",
    "            \"test_pynq_board\": test_pynq_board,\n",
    "            \"target_clk_ns\": target_clk_ns,\n",
    "            \"save_file_prefix\": save_file_prefix,\n",
    "            \"topology\": topology, \n",
    "            \"wbits\": wbits, \n",
    "            \"abits\": abits,\n",
    "            \"folding\": final_folding,\n",
    "            \"print_name\": print_name,\n",
    "            \"rtl_throughput_dict\": rtl_throughput_dict,\n",
    "            \"fifo_max\": fifo_max,\n",
    "            \"stream_stats\": stream_stats,\n",
    "            \"PPR_utilization\": utilization_data,\n",
    "            \"SYNTH_utilization\": report_data,\n",
    "            \"timing_data\": timing_data,\n",
    "            \"HW_throughput_dict\": HW_throughput_dict,\n",
    "            \"max_LUT_utilization_ratio\": max_LUT_utilization_ratio,\n",
    "            \"parameter_priority\": parameter_priority,\n",
    "            \"inital_folding\": inital_folding,\n",
    "            \"auto_set_FIFO_depth\": auto_set_FIFO_depth,\n",
    "            \"pruning_ratio\": pruning_ratio, \n",
    "            \"pruning_mode\": pruning_mode,\n",
    "            \"scan_script_version\": scan_script_version,\n",
    "            \"maximum_folding_improvement\": maximum_folding_improvement,\n",
    "            \"build_error\": build_error,\n",
    "            \"ppr_report_error\": ppr_report_error,\n",
    "            \"rtl_sim_error\": rtl_sim_error,\n",
    "            \"HW_test_error\": HW_test_error,\n",
    "            \"dataflow_model_strings\": dataflow_model_strings,\n",
    "        }\n",
    "        \n",
    "        # Check if the run failed or succeded\n",
    "        run_succeded = True\n",
    "        if no_synth_error == False:\n",
    "            run_succeded = False\n",
    "        \n",
    "        # Update max value if run succeded\n",
    "        if run_succeded:\n",
    "            export_data[\"largest PPRing max_LUT\"] = max_LUT_utilization_ratio\n",
    "        # Append data to export data and save result\n",
    "        export_data[str(max_LUT_utilization_ratio)] = data\n",
    "        print(export_data.keys())\n",
    "        # Add data for resuming a run\n",
    "        export_data['last max_LUT for resuming'] = max_LUT_utilization_ratio\n",
    "        \n",
    "        # Handle differently if this was the first run\n",
    "        if first_run:\n",
    "            if run_succeded:\n",
    "                max_LUT_utilization_ratio += 0.1\n",
    "                print(f\"\\t{print_name}: PPR succeded on first try, running with higher max_LUT_utilization_ratio={max_LUT_utilization_ratio:.2f}\")\n",
    "            else:\n",
    "                max_LUT_utilization_ratio -= 0.1\n",
    "                print(f\"\\t{print_name}: PPR failed on first try, running with lower max_LUT_utilization_ratio={max_LUT_utilization_ratio:.2f}\")\n",
    "            first_run = False\n",
    "            first_run_succeded = run_succeded\n",
    "        else:\n",
    "            # Check if we reached a cross over point\n",
    "            # Or if we are still on the same trajectory, then we need to further increase/decrease the max_LUT_utilization_ratio\n",
    "            if first_run_succeded == run_succeded:\n",
    "                if run_succeded:\n",
    "                    # Stop here if we maxed out the folding\n",
    "                    if maximum_folding_improvement:\n",
    "                        export_data['PPR optimization successfull'] = True\n",
    "                        running = False\n",
    "                        print(f\"\\t{print_name}: PPR succeded on current try and maxed out folding improvements, stopping here. Last max_LUT_utilization_ratio={max_LUT_utilization_ratio:.2f}\")\n",
    "                    else:\n",
    "                        max_LUT_utilization_ratio += 0.1\n",
    "                        print(f\"\\t{print_name}: PPR succeded on current try, running with higher max_LUT_utilization_ratio={max_LUT_utilization_ratio:.2f}\")\n",
    "                else:\n",
    "                    max_LUT_utilization_ratio -= 0.1\n",
    "                    print(f\"\\t{print_name}: PPR failed on current try, running with lower max_LUT_utilization_ratio={max_LUT_utilization_ratio:.2f}\")\n",
    "            else:\n",
    "                # We arrived at the corss over point, so we can stop here.\n",
    "                print(f\"\\t{print_name}: Arrived at cross over point, stoping here. Last max_LUT_utilization_ratio={max_LUT_utilization_ratio:.2f}\")\n",
    "                export_data['PPR optimization successfull'] = True\n",
    "                running = False\n",
    "        \n",
    "        print(f\"\\t{print_name}: Run ended, dumping data\")\n",
    "        result_file_name = build_dir + \"/{}.json.gz\".format(print_name)\n",
    "        with gzip.open(result_file_name, \"wt\") as write_file:\n",
    "            json.dump(export_data, write_file)\n",
    "\n",
    "        result_file_name = \"/workspace/finn\" + \"/{}.json.gz\".format(print_name)\n",
    "        with gzip.open(result_file_name, \"wt\") as write_file:\n",
    "            json.dump(export_data, write_file)\n",
    "        \n",
    "        # check if the run failed over all\n",
    "        # We do this at the top now to catch old settings coming from resuming old runs\n",
    "        #if max_LUT_utilization_ratio <= 0.25:\n",
    "        #    running = False\n",
    "        #    print(f\"\\t{print_name}: PPR failed too many times, ending here.\")\n",
    "\n",
    "    print(\"Finished test for: {}\".format(print_name))\n",
    "\n",
    "    return export_data\n",
    "\n",
    "def read_cpu_target_from_disk(cpu_percentace_max):\n",
    "    with open(build_dir + '/cpu_percentage_max.txt', \"r\") as f:\n",
    "        cpu_percentace_max_new = float(f.read())\n",
    "    if not (cpu_percentace_max == cpu_percentace_max_new):\n",
    "        print(f\"cpu_percentace_max now: {cpu_percentace_max_new:.1f}\")\n",
    "    return cpu_percentace_max_new\n",
    "\n",
    "\n",
    "def read_num_workers_from_disk():\n",
    "    with open(build_dir + '/num_workers.txt', \"r\") as f:\n",
    "        num_workers_new = int(f.read())\n",
    "    return num_workers_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_save_file_prefix(pruning_ratio, pruning_mode, target_clk_ns, auto_set_FIFO_depth):\n",
    "    save_file_prefix = \"\"\n",
    "    if pruning_ratio == None:\n",
    "        save_file_prefix = \"autoTune_None_\"\n",
    "    else:\n",
    "        save_file_prefix = \"autoTune_{:.2f}_{}_\".format(pruning_ratio, pruning_mode)\n",
    "    if auto_set_FIFO_depth == True:\n",
    "        save_file_prefix += \"autoFIFO_\"\n",
    "    \n",
    "    save_file_prefix += f\"clk_ns_{target_clk_ns}\"\n",
    "    \n",
    "    return save_file_prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_dir = \"/workspace/finn\"\n",
    "build_dir = \"/tmp/finn_dev_hendrik\"\n",
    "ENABLE_NETRON = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the matrix of known LUT budgets for unpruned data\n",
    "# Denoted as [W][A]\n",
    "inital_LUT_budget = {\n",
    "    1:{1:1.4, 2:1.2, 3:1.3, 4:1.0, 5:0.7},\n",
    "    2:{2:1.1, 3:1.0, 4:0.9, 5:0.7},\n",
    "    3:{2:0.9, 3:0.9, 4:0.7},\n",
    "    4:{2:0.9, 3:0.9},\n",
    "}\n",
    "inital_LUT_budget_for_unknown = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pynq_board = \"Pynq-Z1\"\n",
    "test_pynq_board = \"Ultra96\"\n",
    "test_fpga_part = pynq_part_map[test_pynq_board]\n",
    "target_clk_ns = 10\n",
    "\n",
    "cpu_percentace_max = 94.0\n",
    "auto_set_FIFO_depth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.25, 0.5, 0.75, 0.875],\n",
       " ['coarse'],\n",
       " [1, 2, 3, 4, 5],\n",
       " [['PE', 'SIMD'], ['balanced']],\n",
       " 200)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruning_ratio_list = [0.25, 0.5, 0.75, 0.875]\n",
    "#pruning_ratio_list = [0.5, 0.75, 0.875]\n",
    "#pruning_ratio_list = [None]\n",
    "\n",
    "pruning_mode_list = [\"coarse\"]\n",
    "#pruning_mode_list = [\"fine\"]\n",
    "#pruning_mode_list = [None]\n",
    "\n",
    "bit_list = list(range(1, 6))\n",
    "\n",
    "#parameter_priority_list = [[\"SIMD\", \"PE\"],[\"PE\", \"SIMD\"],[\"balanced\"]]\n",
    "#parameter_priority_list = [[\"SIMD\", \"PE\"],[\"balanced\"]]\n",
    "parameter_priority_list = [[\"PE\", \"SIMD\"],[\"balanced\"]]\n",
    "\n",
    "pruning_ratio_list, pruning_mode_list, bit_list, parameter_priority_list,  len(parameter_priority_list) * len(pruning_ratio_list) * (len(bit_list) **2) * len(pruning_mode_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parallel implementation\n",
    "\n",
    "thread_list = []\n",
    "HW_lock = Lock()\n",
    "pruning_ratio = None\n",
    "\n",
    "topology=  \"cnv\"\n",
    "i = 0\n",
    "\n",
    "for pruning_mode in pruning_mode_list:\n",
    "    for weight_bit_width in bit_list:\n",
    "        for act_bit_width in bit_list:\n",
    "            for pruning_ratio in pruning_ratio_list:\n",
    "                for parameter_priority in parameter_priority_list:\n",
    "                    # start the next thread only when the required CPU resources become available\n",
    "                    curr_cpu = 1000.\n",
    "                    cpu_percentace_max = read_cpu_target_from_disk(cpu_percentace_max)\n",
    "                    while (curr_cpu > cpu_percentace_max):\n",
    "                        curr_cpu = psutil.cpu_percent(interval = 10.)\n",
    "                        cpu_percentace_max = read_cpu_target_from_disk(cpu_percentace_max)\n",
    "                        num_workers = read_num_workers_from_disk()\n",
    "                        os.environ['NUM_DEFAULT_WORKERS'] = str(num_workers)\n",
    "                    \n",
    "                    # Try to get a sane estimate for the first max_LUT_utilization_ratio_start\n",
    "                    try:\n",
    "                        max_LUT_utilization_ratio_start = inital_LUT_budget[weight_bit_width][act_bit_width]\n",
    "                    except KeyError:\n",
    "                        max_LUT_utilization_ratio_start = inital_LUT_budget_for_unknown\n",
    "                    \n",
    "                    # For FIFO auto tuning, try to start off with already tested stuff\n",
    "                    if auto_set_FIFO_depth:\n",
    "                        save_file_prefix = generate_save_file_prefix(pruning_ratio, pruning_mode, target_clk_ns, False)\n",
    "                        get_checkpoint_name = get_checkpoint_name_generator(build_dir, save_file_prefix, topology, weight_bit_width, act_bit_width, parameter_priority)\n",
    "                        print_name = get_checkpoint_name(\"\", get_full_path=False)\n",
    "                        result_file_name = build_dir + \"/{}.json.gz\".format(print_name)\n",
    "                        if os.path.isfile(result_file_name):\n",
    "                            with gzip.open(result_file_name, 'rb') as json_file:\n",
    "                                finn_data_raw = json.load(json_file)\n",
    "                            max_LUT_utilization_ratio_start = finn_data_raw['largest PPRing max_LUT']\n",
    "                            print(\"Found non-auto FIFO data from: {}, with max_LUT: {}\".format(print_name, max_LUT_utilization_ratio_start))\n",
    "                            # if the previous run had failed, we start out at 0.3, because there is little hope that this works any ways\n",
    "                            if max_LUT_utilization_ratio_start == 0.:\n",
    "                                max_LUT_utilization_ratio_start = 0.3\n",
    "                    \n",
    "                    \n",
    "                    # Check if the output file already exists\n",
    "                    save_file_prefix = generate_save_file_prefix(pruning_ratio, pruning_mode, target_clk_ns, auto_set_FIFO_depth)\n",
    "                    get_checkpoint_name = get_checkpoint_name_generator(build_dir, save_file_prefix, topology, weight_bit_width, act_bit_width, parameter_priority)\n",
    "                    print_name = get_checkpoint_name(\"\", get_full_path=False)\n",
    "                    result_file_name = build_dir + \"/{}.json.gz\".format(print_name)\n",
    "                    start_export_data = None\n",
    "                    if os.path.isfile(result_file_name):\n",
    "                        print(\"Already exists for: {}\".format(print_name))\n",
    "                        # Check if we can run off of this data or if it was alrady completed\n",
    "                        with gzip.open(result_file_name, 'rb') as json_file:\n",
    "                            finn_data_raw = json.load(json_file)\n",
    "                        # Handle old data format\n",
    "                        try:\n",
    "                            finished = finn_data_raw['PPR optimization successfull']\n",
    "                        except KeyError:\n",
    "                            finished = finn_data_raw[\"Finished PPR optimization\"]\n",
    "                        \n",
    "                        if finished:\n",
    "                            print(\"Already finished for: {}, continuing\".format(print_name))\n",
    "                            continue\n",
    "                        else:\n",
    "                            # Check if the optimization ended in a 0, then it's also finished\n",
    "                            if finn_data_raw['largest PPRing max_LUT'] == 0.:\n",
    "                                print(\"Already unseccessfully finished for: {}, continuing\".format(print_name))\n",
    "                                continue\n",
    "                            else:\n",
    "                                # resume the optimization here\n",
    "                                start_export_data = finn_data_raw\n",
    "                                if 'last max_LUT for resuming' in finn_data_raw.keys():\n",
    "                                    max_LUT_utilization_ratio_start = finn_data_raw['last max_LUT for resuming']\n",
    "                                else:\n",
    "                                    lut_list = []\n",
    "                                    for key in finn_data_raw.keys():\n",
    "                                        try:\n",
    "                                            lut_list.append(float(key))\n",
    "                                        except ValueError:\n",
    "                                            pass\n",
    "                                    if max(lut_list) > max_LUT_utilization_ratio_start:\n",
    "                                        max_LUT_utilization_ratio_start = max(lut_list)\n",
    "                                    else:\n",
    "                                        max_LUT_utilization_ratio_start = min(lut_list)\n",
    "                                    print(\"Resuming for: {}, with max_LUT_utilization_ratio_start: {}\".format(print_name, max_LUT_utilization_ratio_start))\n",
    "                    \n",
    "                    # kick off the next thread\n",
    "                    num_workers = read_num_workers_from_disk()\n",
    "                    os.environ['NUM_DEFAULT_WORKERS'] = str(num_workers)\n",
    "                    args = (pruning_ratio, pruning_mode, weight_bit_width, act_bit_width, target_clk_ns, test_pynq_board, HW_lock, build_dir, topology)\n",
    "                    kwargs = {\n",
    "                        \"parameter_priority\": parameter_priority,\n",
    "                        \"max_LUT_utilization_ratio_start\": max_LUT_utilization_ratio_start,\n",
    "                        \"auto_set_FIFO_depth\": auto_set_FIFO_depth,\n",
    "                        \"start_export_data\": start_export_data\n",
    "                    }\n",
    "                    print(\"Starting from main thread: {}\".format(print_name))\n",
    "                    t = Process(target=run_one_test, args=args, kwargs=kwargs)\n",
    "                    t.start()\n",
    "                    thread_list.append(t)\n",
    "                    i += 1\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processes running: 0      \n",
      "All threads finished\n"
     ]
    }
   ],
   "source": [
    "# Wait for all threads to complete\n",
    "number_p_alive = 1\n",
    "while number_p_alive > 0:\n",
    "    number_p_alive = 0\n",
    "    for t in thread_list:\n",
    "        if t.is_alive():\n",
    "            number_p_alive += 1\n",
    "    print(f\"Processes running: {number_p_alive}     \", end=\"\\r\")\n",
    "    time.sleep(1.)\n",
    "\n",
    "# Join all threads\n",
    "for t in thread_list: \n",
    "    t.join()\n",
    "print(\"\")\n",
    "print(\"All threads finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serial implementation for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serial implementation\n",
    "HW_lock = Lock()\n",
    "test_pynq_board = \"Ultra96\"\n",
    "target_clk_ns = 10\n",
    "topology, weight_bit_width, act_bit_width =  \"cnv\", 1, 1\n",
    "\n",
    "#pruning_ratio = 0.5\n",
    "pruning_ratio = None\n",
    "#pruning_mode = \"fine\"\n",
    "pruning_mode = None\n",
    "\n",
    "parameter_priority = parameter_priority_list[2]\n",
    "max_LUT_utilization_ratio_start = 1.4\n",
    "\n",
    "num_workers = read_num_workers_from_disk()\n",
    "#os.environ['NUM_DEFAULT_WORKERS'] = str(num_workers)\n",
    "os.environ['NUM_DEFAULT_WORKERS'] = str(20)\n",
    "print(os.environ['NUM_DEFAULT_WORKERS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = run_one_test(pruning_ratio, pruning_mode, weight_bit_width, act_bit_width, target_clk_ns, \n",
    "                    test_pynq_board, HW_lock, build_dir, topology, \n",
    "                    timeout=120, max_LUT_utilization_ratio_start = max_LUT_utilization_ratio_start,\n",
    "                    parameter_priority = parameter_priority,\n",
    "                    auto_set_FIFO_depth = True\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
